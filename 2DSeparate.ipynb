{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e57e553",
   "metadata": {},
   "source": [
    "# Colab Torch Test\n",
    "## Chris Tralie\n",
    "\n",
    "A simple test of torch in colab running logistic regression on two approximately linearly separable blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e30ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5df0fb9",
   "metadata": {},
   "source": [
    "# Torch Data Loader for 2D Blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00948d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Blobs(Dataset):\n",
    "    def __init__(self, n_samples, dim, offset):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_samples: int\n",
    "            Number of samples to create per epoch\n",
    "        dim: int\n",
    "            Dimension of blobs\n",
    "        offset: ndarray(dim)\n",
    "            Offset of first blob with respect to the second one\n",
    "        \"\"\"\n",
    "        self.n_samples = n_samples\n",
    "        self.dim = dim\n",
    "        self.offset = np.ones(dim)*offset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y = np.round(np.random.rand())\n",
    "        x = np.random.randn(self.dim)\n",
    "        if y == 1:\n",
    "            x += self.offset\n",
    "        x = torch.from_numpy(np.array(x, dtype=np.float32))\n",
    "        y = torch.from_numpy(np.array(y, dtype=np.float32))\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece5c178",
   "metadata": {},
   "source": [
    "## Setup blobs dataset and solve logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92072e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device\", device)\n",
    "\n",
    "n_samples = 1000\n",
    "dim = 2\n",
    "offset = 2\n",
    "data = Blobs(n_samples, dim, 2)\n",
    "\n",
    "perceptron = nn.Linear(dim, 1)\n",
    "perceptron = perceptron.to(device)\n",
    "optimizer = torch.optim.Adam(perceptron.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "n_epochs = 50\n",
    "losses = []\n",
    "accuracy = []\n",
    "plt.figure(figsize=(12, 9))\n",
    "for epoch in range(n_epochs):\n",
    "    loader = DataLoader(data, batch_size=16, shuffle=True)\n",
    "    for i, (inputs, labels) in enumerate(loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = perceptron(inputs)\n",
    "        loss = loss_fn(outputs[:, 0], labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Look at results on test set\n",
    "    test_loader = DataLoader(data, batch_size=len(data))\n",
    "    inputs, labels = next(iter(test_loader))\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = perceptron(inputs)\n",
    "    total_loss = loss_fn(outputs[:, 0], labels)\n",
    "    num_correct = torch.sum(0.5*(torch.sign(outputs[:, 0])+1) == labels)\n",
    "    losses.append(total_loss.item())\n",
    "    accuracy.append(num_correct.cpu()/len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdcedbb",
   "metadata": {},
   "source": [
    "## Plot Optimization Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc22423",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.plot(losses)\n",
    "plt.title(\"Losses\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.subplot(212)\n",
    "plt.plot(np.array(accuracy)*100)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2a4de9",
   "metadata": {},
   "source": [
    "## Extract Parameters And Plot Some Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc03c297",
   "metadata": {},
   "outputs": [],
   "source": [
    "[ab, c] = list(perceptron.parameters())\n",
    "ab = ab.detach().cpu().numpy().flatten()\n",
    "a, b = ab\n",
    "c = c.detach().cpu().numpy().flatten()[0]\n",
    "print(a, b, c)\n",
    "\n",
    "\n",
    "X0 = []\n",
    "X1 = []\n",
    "for i in range(200):\n",
    "    x, y = data[i]\n",
    "    x = x.detach().cpu().numpy()\n",
    "    if y == 0:\n",
    "        X0.append(x)\n",
    "    else:\n",
    "        X1.append(x)\n",
    "X0 = np.array(X0)\n",
    "X1 = np.array(X1)\n",
    "\n",
    "logistic = lambda u: 1/(1 + np.exp(-u))\n",
    "\n",
    "\n",
    "def plot_logistic_regression_predictions(X1, X2, a, b, c):\n",
    "    plt.scatter(X1[:, 0], X1[:, 1])\n",
    "    plt.scatter(X2[:, 0], X2[:, 1])\n",
    "    X = np.concatenate((X1, X2), axis=0)\n",
    "    xmin = np.min(X, axis=0)\n",
    "    xmax = np.max(X, axis=0)\n",
    "    iv = max(xmax[1]-xmin[1], xmax[0]-xmin[0])\n",
    "    \n",
    "    resol = 200\n",
    "    mx = 2\n",
    "    xx = np.linspace(xmin[0], xmax[0], resol)\n",
    "    yy = np.linspace(xmin[1], xmax[1], resol)\n",
    "    xx, yy = np.meshgrid(xx, yy)\n",
    "    plt.imshow(logistic(a*xx+b*yy+c), extent=(xmin[0], xmax[0], xmax[1], xmin[1]), cmap='RdBu_r', vmin=0, vmax=1)\n",
    "    \n",
    "    \n",
    "    p0 = -c*np.array([a, b])/(a**2 + b**2)\n",
    "    v = np.array([-b, a])\n",
    "    mag = np.sqrt(np.sum(v**2))\n",
    "    if mag > 0:\n",
    "        v = v/mag\n",
    "        p = p0 - iv*v\n",
    "        q = p0 + iv*v\n",
    "        plt.plot([p[0], q[0]], [p[1], q[1]])\n",
    "        rg = xmax[0] - xmin[0]\n",
    "        plt.xlim([xmin[0]-0.2*rg, xmax[0]+0.2*rg])\n",
    "        rg = xmax[1] - xmin[1]\n",
    "        plt.ylim([xmin[1]-0.2*rg, xmax[1]+0.2*rg])\n",
    "\n",
    "        wrong = 0\n",
    "        for x in X1:\n",
    "            y = logistic(a*x[0] + b*x[1] + c)\n",
    "            proj = p0 + np.sum(v*(x-p0))*v\n",
    "            #plt.plot([x[0], proj[0]], [x[1], proj[1]], c='C0')\n",
    "            if y > 0.5:\n",
    "                plt.scatter([x[0]], [x[1]], 200, c='C0', marker='x')\n",
    "                wrong += 1\n",
    "        for x in X2:\n",
    "            y = logistic(a*x[0] + b*x[1] + c)\n",
    "            proj = p0 + np.sum(v*(x-p0))*v\n",
    "            #plt.plot([x[0], proj[0]], [x[1], proj[1]], c='C1')\n",
    "            if y < 0.5:\n",
    "                plt.scatter([x[0]], [x[1]], 200, c='C1', marker='x')\n",
    "                wrong += 1\n",
    "        N = X.shape[0]\n",
    "        plt.title(\"a = {:.3f}, b = {:.3f}, c = {:.3f}\\n{} Wrong ({} % Accuracy)\".format(a, b, c, wrong, int(100*(N-wrong)/N)))\n",
    "        plt.axis(\"equal\")\n",
    "\n",
    "plot_logistic_regression_predictions(X0, X1, a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e5656b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
